---
title: "Final Project"
output: html_document
date: "2024-12-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
```{r}

```

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
library(readr)
library(gmodels)
library(GGally)
library(caret)
library(ggcorrplot)
library(car)
library(corrplot)
library(glmnet)

```



```{r}

# Load the dataset
data <- read.csv("Admission_Predict_Ver1.1.csv")

print(sum(is.na(data)))

print(sum(duplicated(data)))

set.seed(123)

data$University.Rating <- as.factor(data$University.Rating)
data$Research <- as.factor(data$Research)

par(mfrow = c(2, 3))

features <- names(data)
features
```

```{r}


for (feature in features) {
  # 判断是连续变量还是分类变量
  if (is.numeric(data[[feature]])) {
    # 连续变量的直方图
    hist(data[[feature]], main = paste("Histogram of", feature), xlab = feature, col = "skyblue", border = "black")
    # 连续变量的箱线图
    boxplot(data[[feature]], main = paste("Boxplot of", feature), ylab = feature, col = "orange")
  } else {
    # 分类变量的柱状图
    barplot(table(data[[feature]]), main = paste("Barplot of", feature), col = "skyblue", xlab = feature, ylab = "Frequency")
    # 分类变量的箱线图
    boxplot(data$Chance.of.Admit ~ data[[feature]], main = paste(feature, "vs Chance of Admit"),
            xlab = feature, ylab = "Chance of Admit", col = "lightgreen")
  }
}

# 绘制每个特征与 Chance of Admit 的散点图（连续变量）或箱线图（分类变量）
for (feature in features) {
  if (is.numeric(data[[feature]])) {
    # 连续变量的散点图
    plot(data[[feature]], data$Chance.of.Admit, main = paste(feature, "vs Chance of Admit"),
         xlab = feature, ylab = "Chance of Admit", col = "blue", pch = 19)
  } else {
    # 分类变量的箱线图
    boxplot(data$Chance.of.Admit ~ data[[feature]], main = paste(feature, "vs Chance of Admit"),
            xlab = feature, ylab = "Chance of Admit", col = "lightgreen")
  }
}

```



```{r}
# 提取连续变量
numeric_features <- data[, -1]
numeric_features <- numeric_features[, sapply(numeric_features, is.numeric)]

# 计算相关矩阵
cor_matrix <- cor(numeric_features)

# 打印相关矩阵
print(cor_matrix)

# 可视化相关矩阵

corrplot(cor_matrix, method = "color", type = "upper", tl.col = "black", tl.srt = 45, addCoef.col = "black")

```




#Linear Model
```{r}
#basic model
linear_model <- lm(Chance.of.Admit ~ ., data = data[, -1])
summary(linear_model)

```


#Polynomial Model
```{r}
#polynomial model
poly_model<-lm(Chance.of.Admit ~ GRE.Score + I(GRE.Score^2) +
                   TOEFL.Score + I(TOEFL.Score^2) +
                   CGPA + I(CGPA^2)  +
                   SOP + LOR + Research, data = data)
summary(poly_model)
```

#Interaction Model
```{r}
interaction_model <- lm(Chance.of.Admit ~ GRE.Score * University.Rating +
                          CGPA * Research +
                          TOEFL.Score + SOP + LOR, data = data)
summary(interaction_model)
```

#PCA model
```{r}
# 使用 preProcess 进行 PCA 并提取 3 个主成分
pca_lm <- preProcess(data[, c("GRE.Score", "TOEFL.Score", "CGPA", "SOP", "LOR")], method = "pca", pcaComp = 3)
pca_lm_data <- predict(pca_lm, data)

# 修改列名以确保与公式中一致
colnames(pca_lm_data) <- c("PC1", "PC2", "PC3")

# 合并 PCA 主成分和原始数据
pca_combined_data <- cbind(data, pca_lm_data)

# 构建 PCA 线性模型
model_pca <- lm(Chance.of.Admit ~ PC1 + PC2 + PC3 + University.Rating + Research, data = pca_combined_data)
summary(model_pca)

# 计算 PCA 线性模型指标
pca_pred <- predict(model_pca, pca_combined_data)  # 预测值
pca_r2 <- summary(model_pca)$r.squared  # R2
pca_mse <- mean((data$Chance.of.Admit - pca_pred)^2)  # MSE
pca_rse <- sqrt(pca_mse)  # RSE
pca_aic <- AIC(model_pca)  # AIC
pca_bic <- BIC(model_pca)  # BIC

```

#Ridge Model
```{r}

# 准备数据
ridge <- cv.glmnet(as.matrix(data[, -c(1, 9)]), data$Chance.of.Admit, alpha = 0)
ridge_pred <- predict(ridge, s = ridge$lambda.min, newx = as.matrix(data[, -c(1, 9)]))


```

#PCA Ridge
```{r}
pca_result <- prcomp(numeric_features, scale. = TRUE)
summary(pca_result)


# 提取主成分并训练模型
pca_data <- as.data.frame(pca_result$x[, 1:2])
colnames(pca_data) <- c("PC1", "PC2")
pca_data$Chance.of.Admit <- data$Chance.of.Admit

# Ridge 回归使用 PCA
X_pca <- as.matrix(pca_data[, -3])
y_pca <- pca_data$Chance.of.Admit
ridge_pca <- cv.glmnet(X_pca, y_pca, alpha = 0, nfolds = 5)
ridge_pca_pred <- predict(ridge_pca, s = ridge_pca$lambda.min, newx = X_pca)
ridge_pca_mse <- mean((y_pca - ridge_pca_pred)^2)



ridge_pca_r2 <- 1 - sum((y_pca - ridge_pca_pred)^2) / sum((y_pca - mean(y_pca))^2)  # R2
ridge_pca_rse <- sqrt(mean((y_pca - ridge_pca_pred)^2))  # Residual Standard Error
ridge_pca_mse <- mean((y_pca - ridge_pca_pred)^2)  # MSE

n_pca <- length(y_pca)  # 样本数
p_pca <- ncol(X_pca)  # 主成分数量
ridge_pca_aic <- n_pca * log(ridge_pca_mse) + 2 * p_pca  # AIC
ridge_pca_bic <- n_pca * log(ridge_pca_mse) + log(n_pca) * p_pca  # BIC

# 打印 PCA 模型的结果
cat("PCA Model Metrics:\n")
cat("R2:", ridge_pca_r2, "\n")
cat("MSE:", ridge_pca_mse, "\n")
cat("Residual Standard Error:", ridge_pca_rse, "\n")
cat("AIC:", ridge_pca_aic, "\n")
cat("BIC:", ridge_pca_bic, "\n")
```



#Ridge Linear, Ridge Poly, Ridge Interaction
```{r}

X <- model.matrix(Chance.of.Admit ~ ., data = data[, -1])
y <- data$Chance.of.Admit

# 使用 Ridge 回归
ridge_linear <- cv.glmnet(X, y, alpha = 0, nfolds = 5)

# 构建多项式特征
poly_features <- model.matrix(~ GRE.Score + I(GRE.Score^2) + TOEFL.Score + I(TOEFL.Score^2) +
                                CGPA + I(CGPA^2) + I(CGPA^3) + SOP + LOR + Research, data)

ridge_poly <- cv.glmnet(poly_features, data$Chance.of.Admit, alpha = 0)

# 构建交互特征
interaction_features <- model.matrix(~ GRE.Score * University.Rating + CGPA * Research + 
                                       TOEFL.Score + SOP + LOR, data)

ridge_interaction <- cv.glmnet(interaction_features, data$Chance.of.Admit, alpha = 0)


```


#Calculate all AIC BIC MSE R2 RSE
```{r}
# Ridge Model Metrics


ridge_r2 <- 1 - sum((y - ridge_pred)^2) / sum((y - mean(y))^2)  # R2
ridge_rse <- sqrt(mean((y - ridge_pred)^2))  # Residual Standard Error
ridge_mse <- mean((y - ridge_pred)^2)  # MSE

ridge_coef <- coef(ridge, s = ridge$lambda.min)
n <- length(y)  # 样本数
p <- length(ridge_coef) - 1  # 参数数量，减去截距
ridge_aic <- n * log(mean((y - ridge_pred)^2)) + 2 * p  # AIC
ridge_bic <- n * log(mean((y - ridge_pred)^2)) + log(n) * p  # BIC

######################################
# Linear Model Metrics
linear_pred <- predict(linear_model, data)

linear_r2 <- summary(linear_model)$r.squared
linear_rse <- summary(linear_model)$sigma
linear_mse <- mean((data$Chance.of.Admit - linear_pred)^2)  # MSE
linear_aic <- AIC(linear_model)
linear_bic <- BIC(linear_model)

#######################################
# Polynomial Model Metrics
poly_pred <- predict(poly_model, data)

poly_r2 <- 1 - sum((data$Chance.of.Admit - poly_pred)^2) / sum((data$Chance.of.Admit - mean(data$Chance.of.Admit))^2)
poly_rse <- sqrt(mean((data$Chance.of.Admit - poly_pred)^2))
poly_mse <- mean((data$Chance.of.Admit - poly_pred)^2)  # MSE
poly_aic <- AIC(poly_model)
poly_bic <- BIC(poly_model)

########################################
# Interaction Model Metrics
interaction_pred <- predict(interaction_model, data)

interaction_r2 <- 1 - sum((data$Chance.of.Admit - interaction_pred)^2) / sum((data$Chance.of.Admit - mean(data$Chance.of.Admit))^2)
interaction_rse <- sqrt(mean((data$Chance.of.Admit - interaction_pred)^2))
interaction_mse <- mean((data$Chance.of.Admit - interaction_pred)^2)  # MSE
interaction_aic <- AIC(interaction_model)
interaction_bic <- BIC(interaction_model)

########################################

ridge_linear_pred <- predict(ridge_linear, s = ridge_linear$lambda.min, newx = X)
# 计算指标
ridge_linear_r2 <- 1 - sum((data$Chance.of.Admit - ridge_linear_pred)^2) / sum((data$Chance.of.Admit - mean(data$Chance.of.Admit))^2)
ridge_linear_mse <- mean((data$Chance.of.Admit - ridge_linear_pred)^2)
ridge_linear_rse <- sqrt(ridge_linear_mse)

n <- nrow(data)  # 样本数
p <- ncol(data[, -c(1, 9)])  # 特征数量
ridge_linear_aic <- n * log(ridge_linear_mse) + 2 * p
ridge_linear_bic <- n * log(ridge_linear_mse) + log(n) * p
########################################
poly_features <- model.matrix(~ GRE.Score + I(GRE.Score^2) + TOEFL.Score + I(TOEFL.Score^2) +
                                CGPA + I(CGPA^2) + I(CGPA^3) + SOP + LOR + Research, data)

# Ridge 多项式模型
ridge_poly <- cv.glmnet(poly_features, data$Chance.of.Admit, alpha = 0)

# Ridge 多项式模型预测值
ridge_poly_pred <- predict(ridge_poly, s = ridge_poly$lambda.min, newx = poly_features)

# 计算指标
ridge_poly_r2 <- 1 - sum((data$Chance.of.Admit - ridge_poly_pred)^2) / sum((data$Chance.of.Admit - mean(data$Chance.of.Admit))^2)
ridge_poly_mse <- mean((data$Chance.of.Admit - ridge_poly_pred)^2)
ridge_poly_rse <- sqrt(ridge_poly_mse)

p <- ncol(poly_features)  # 特征数量
ridge_poly_aic <- n * log(ridge_poly_mse) + 2 * p
ridge_poly_bic <- n * log(ridge_poly_mse) + log(n) * p
########################################
interaction_features <- model.matrix(~ GRE.Score * University.Rating + CGPA * Research +
                                       TOEFL.Score + SOP + LOR, data)

# Ridge 交互模型
ridge_interaction <- cv.glmnet(interaction_features, data$Chance.of.Admit, alpha = 0)

# Ridge 交互模型预测值
ridge_interaction_pred <- predict(ridge_interaction, s = ridge_interaction$lambda.min, newx = interaction_features)

# 计算指标
ridge_interaction_r2 <- 1 - sum((data$Chance.of.Admit - ridge_interaction_pred)^2) / sum((data$Chance.of.Admit - mean(data$Chance.of.Admit))^2)
ridge_interaction_mse <- mean((data$Chance.of.Admit - ridge_interaction_pred)^2)
ridge_interaction_rse <- sqrt(ridge_interaction_mse)

p <- ncol(interaction_features)  # 特征数量
ridge_interaction_aic <- n * log(ridge_interaction_mse) + 2 * p
ridge_interaction_bic <- n * log(ridge_interaction_mse) + log(n) * p
########################################




results <- data.frame(
  Model = c("Linear", "Polynomial", "Interaction", "Ridge", "Ridge Linear", 
            "Ridge Polynomial", "Ridge Interaction", "PCA Linear", "PCA + Ridge"),
  R2 = c(linear_r2, poly_r2, interaction_r2, ridge_r2, ridge_linear_r2, ridge_poly_r2, ridge_interaction_r2, pca_r2, ridge_pca_r2),
  MSE = c(linear_mse, poly_mse, interaction_mse, ridge_mse, ridge_linear_mse, ridge_poly_mse, ridge_interaction_mse, pca_mse, ridge_pca_mse),
  RSE = c(linear_rse, poly_rse, interaction_rse, ridge_rse, ridge_linear_rse, ridge_poly_rse, ridge_interaction_rse, pca_rse, ridge_pca_rse),
  AIC = c(linear_aic, poly_aic, interaction_aic, ridge_aic, ridge_linear_aic, ridge_poly_aic, ridge_interaction_aic, pca_aic, ridge_pca_aic),
  BIC = c(linear_bic, poly_bic, interaction_bic, ridge_bic, ridge_linear_bic, ridge_poly_bic, ridge_interaction_bic, pca_bic, ridge_pca_bic)
)

# 打印结果表格
print(results)




```

#plot
```{r}

# 检查预测值是否正确生成
cat("Length of actual values:", length(data$Chance.of.Admit), "\n")
cat("Length of linear_pred:", length(linear_pred), "\n")
cat("Length of poly_pred:", length(poly_pred), "\n")
cat("Length of interaction_pred:", length(interaction_pred), "\n")
cat("Length of ridge_pred:", length(ridge_pred), "\n")
cat("Length of ridge_linear_pred:", length(ridge_linear_pred), "\n")
cat("Length of ridge_poly_pred:", length(ridge_poly_pred), "\n")
cat("Length of ridge_interaction_pred:", length(ridge_interaction_pred), "\n")
cat("Length of pca_pred:", length(pca_pred), "\n")
cat("Length of ridge_pca_pred:", length(ridge_pca_pred), "\n")

plot_model <- function(actual, predicted, title) {
  if (length(actual) != length(predicted)) {
    stop("Actual and predicted values have different lengths!")
  }
  
  # 创建数据框
  data_plot <- data.frame(Actual = actual, Predicted = as.numeric(predicted))
  
  # 绘制散点图
  ggplot(data = data_plot, aes(x = Actual, y = Predicted)) +
    geom_point(color = "blue", alpha = 0.6) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red", size = 1) +
    labs(title = title, x = "Actual Chance of Admit", y = "Predicted Chance of Admit") +
    theme_minimal()
}

# 确保预测值是向量（例如 ridge_pred 可能是矩阵，需转换为向量）
ridge_pred <- as.numeric(ridge_pred)
ridge_linear_pred <- as.numeric(ridge_linear_pred)
ridge_poly_pred <- as.numeric(ridge_poly_pred)
ridge_interaction_pred <- as.numeric(ridge_interaction_pred)
pca_pred <- as.numeric(pca_pred)
ridge_pca_pred <- as.numeric(ridge_pca_pred)

# 绘制每个模型的图表
plot_list <- list(
  plot_model(data$Chance.of.Admit, linear_pred, "Linear Model"),
  plot_model(data$Chance.of.Admit, poly_pred, "Polynomial Model"),
  plot_model(data$Chance.of.Admit, interaction_pred, "Interaction Model"),
  plot_model(data$Chance.of.Admit, ridge_pred, "Ridge Model"),
  plot_model(data$Chance.of.Admit, ridge_linear_pred, "Ridge Linear Model"),
  plot_model(data$Chance.of.Admit, ridge_poly_pred, "Ridge Polynomial Model"),
  plot_model(data$Chance.of.Admit, ridge_interaction_pred, "Ridge Interaction Model"),
  plot_model(data$Chance.of.Admit, pca_pred, "PCA Linear Model"),
  plot_model(data$Chance.of.Admit, ridge_pca_pred, "PCA + Ridge Model")
)

# 使用循环逐一显示每个图表
for (i in seq_along(plot_list)) {
  print(plot_list[[i]])
}


```
```{r}
# 加载必要的库
library(glmnet)

# PCA 主成分提取
pca_result <- prcomp(numeric_features, scale. = TRUE)
pca_data <- as.data.frame(pca_result$x[, 1:3])  # 提取前 3 个主成分
colnames(pca_data) <- c("PC1", "PC2", "PC3")
pca_data$Chance.of.Admit <- data$Chance.of.Admit  # 添加目标变量

# 准备数据
X_pca <- as.matrix(pca_data[, -4])  # 主成分数据
y_pca <- pca_data$Chance.of.Admit   # 目标变量

# 设置交叉验证
set.seed(123)
ridge_pca_cv <- cv.glmnet(X_pca, y_pca, alpha = 0, nfolds = 5)  # 5 折交叉验证

# 打印交叉验证结果
cat("Best Lambda from PCA Ridge Model:", ridge_pca_cv$lambda.min, "\n")

# 绘制交叉验证误差曲线
plot(ridge_pca_cv)
title("Cross-Validation Error for PCA + Ridge")


# 预测训练集
ridge_pca_train_pred <- predict(ridge_pca_cv, s = ridge_pca_cv$lambda.min, newx = X_pca)

# 计算训练误差
ridge_pca_train_mse <- mean((y_pca - ridge_pca_train_pred)^2)
cat("Training MSE for PCA + Ridge:", ridge_pca_train_mse, "\n")


# 验证集误差（来自 cv.glmnet 的输出）
ridge_pca_val_mse <- min(ridge_pca_cv$cvm)  # 最小验证误差
cat("Validation MSE for PCA + Ridge:", ridge_pca_val_mse, "\n")


# 可视化训练误差和验证误差
error_df <- data.frame(
  Type = c("Training MSE", "Validation MSE"),
  MSE = c(ridge_pca_train_mse, ridge_pca_val_mse)
)

library(ggplot2)
ggplot(error_df, aes(x = Type, y = MSE, fill = Type)) +
  geom_bar(stat = "identity", color = "black") +
  labs(title = "Training vs Validation MSE for PCA + Ridge",
       x = "Error Type", y = "Mean Squared Error") +
  theme_minimal() +
  scale_fill_manual(values = c("skyblue", "orange"))


```







#CGPA vs chance存在非线性关系尤其在高分段有饱和 GRE和TOEFL也是 CGPA，TOEFL，GRE有非线性关系

#从 SOP 和 LOR 的散点图来看，这些特征与 Chance of Admit 之间的关系较为分散，可能说明它们的影响较弱。

#从柱状图中可以看出有research的和university rating高的有更高的chance of admit,
#所以引入交互项

#整个代码没有严重的异常值不需要处理异常值，LOR有一个值是1是一个异常值但是我觉得是真实情况不用处理

#分布比较集中，应该不需要transform

#确定分类变量对录取概率的影响，初步探索数据特征

#SOP增长可能是非线性得，可能需要使用多项式回归

#LOR与其他变量相关性较低，或许是相对独立因素

#we can see that CGPA and GRE score have highest correlation with chance of admit,说明它们对录取概率的影响可能非常显著。toefl也是

#这些特征可以作为主要预测变量，甚至可能在多项式模型中加入非线性项

#LOR和SOP与其他特征的相关性较低，可能因为他比较独立

#通过逻辑推测，GRE和TOEFL可能存在交互关系，因为都反应申请人的学术能力

#GRE和TOEFL相关性高，CGPA和GRE和TOEFL都很高，有可能存在多重共线，后续会考虑PCA和Ridge



#根据qq图怀疑残差不是正态分布，使用shapiro test进行检测确定残差不是正态分布,这对推断如t test，CI会产生影响
#但是如果我们的主要目标是预测而不是推断，正态性的偏离可能不会显著影响模型预测能力
#从qq图来看，残差得偏离主要集中在尾部，实际上轻微的尾部偏离对线性回归的影响可能并不大
#如果有顾虑可以考虑transform data
#transform 后结果更差
#However, given the nature of the deviation and its mild impact on overall model performance, such adjustments may not be critical.
#adjusted R^2 us actually high in predictive accuracy, so may good to not transform or looking for new models.







