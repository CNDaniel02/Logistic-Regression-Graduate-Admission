---
title: "Final Project"
output: html_document
date: "2024-12-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
```{r}

```

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
library(readr)
library(gmodels)
library(GGally)
library(caret)
library(ggcorrplot)
library(car)
library(corrplot)
library(glmnet)
```



```{r}

# load data
data<-read.csv("Admission_Predict_Ver1.1.csv")

#0 na
print(sum(is.na(data)))

#0 duplicate
print(sum(duplicated(data)))

#set random seed for later cross validation
set.seed(42)

# process category variables
data$University.Rating <- as.factor(data$University.Rating)
data$Research <- as.factor(data$Research)

features <- names(data[0,-1])
#features
```

```{r}

for(feature in features) {
  #distinguish continuous and categorical variables
  if (is.numeric(data[[feature]])) {
    #continuous histogram
    hist(data[[feature]],main=paste("Histogram of",feature),xlab=feature,col="skyblue",border="black")
    #boxplot
    boxplot(data[[feature]],main=paste("Boxplot of",feature),ylab=feature,col= "orange")
  }
  else {
    #catego histogram
    barplot(table(data[[feature]]),main=paste("Barplot of",feature),col="skyblue",xlab= feature,ylab="Frequency")
    #boxplot
    boxplot(data$Chance.of.Admit~data[[feature]],main=paste(feature,"vs Chance of Admit"),xlab=feature,ylab="Chance of Admit",col= "lightgreen")
  }
}

#scatter or box of features vs chance
for(feature in features) {
  if(is.numeric(data[[feature]])){
    #continuous scatter
    plot(data[[feature]],data$Chance.of.Admit, main=paste(feature,"vs Chance of Admit"),xlab= feature,ylab="Chance of Admit", col="blue",pch=19)#pch fill
  } 
  else{
    #catego box
    boxplot(data$Chance.of.Admit~data[[feature]],main=paste(feature,"vs Chance of Admit"),xlab=feature,ylab="Chance of Admit",col="lightgreen")
  }
}

```


```{r}
#get continous variables
num_features<-data[,-1] #exclude serial number
num_features<-num_features[,sapply(num_features,is.numeric)]

#correlation matrix 
cor_matrix<-cor(num_features)
corrplot(cor_matrix,method="color",type="upper",tl.col="black",tl.srt=45,addCoef.col="black")

```




#Linear Model
```{r}
#basic model
linear_model<-lm(Chance.of.Admit~.,data=data[,-1])
summary(linear_model)
```


#Polynomial Model
```{r}
#polynomial model
poly_model<-lm(Chance.of.Admit~GRE.Score+I(GRE.Score^2)+TOEFL.Score+I(TOEFL.Score^2)+CGPA+I(CGPA^2)+SOP+LOR+Research,data=data[,-1])
summary(poly_model)
```

#Interaction Model
```{r}
inter_model<-lm(Chance.of.Admit~GRE.Score*University.Rating+CGPA*Research+TOEFL.Score+SOP+LOR,data=data[,-1])
summary(inter_model)
```

#PCA model
```{r}
#preprocess to PCA, with 3 components
pca_lm<-preProcess(data[,c("GRE.Score","TOEFL.Score","CGPA","SOP","LOR")],method="pca",pcaComp=3)
pca_lm_data<-predict(pca_lm,data)

# change variable names for later formula calculation
colnames(pca_lm_data)<-c("PC1","PC2","PC3")

#combine pca components and data
pca_cbdata <- cbind(data, pca_lm_data)

#model implement
model_pca<-lm(Chance.of.Admit~PC1+PC2+PC3+University.Rating+Research,data=pca_cbdata)
summary(model_pca)

#statistical value for later model comparison
pca_pred<-predict(model_pca,pca_cbdata)
pca_r2<-summary(model_pca)$r.squared
pca_mse<-mean((data$Chance.of.Admit-pca_pred)^2)
pca_rse<-sqrt(pca_mse)
pca_aic<-AIC(model_pca) 
pca_bic<-BIC(model_pca)  

#reference: https://rstudio-pubs-static.s3.amazonaws.com/92006_344e916f251146daa0dc49fef94e2104.html?utm_source=chatgpt.com
```

#Ridge Regression Model
```{r}


ridge<-cv.glmnet(as.matrix(data[, -c(1, 9)]),data$Chance.of.Admit,alpha = 0)
ridge_pred<-predict(ridge,s=ridge$lambda.min,newx=as.matrix(data[,-c(1,9)]))


#reference: https://glmnet.stanford.edu/articles/glmnet.html
```

#PCA Ridge
```{r}
pca_result<-prcomp(num_features,scale.=TRUE)
summary(pca_result)

pca_data<-as.data.frame(pca_result$x[,1:2])
colnames(pca_data)<-c("PC1","PC2")
pca_data$Chance.of.Admit<-data$Chance.of.Admit

X_pca<-as.matrix(pca_data[,-3])
y_pca<-pca_data$Chance.of.Admit
ridge_pca<-cv.glmnet(X_pca,y_pca,alpha=0,nfolds=5)
ridge_pca_pred<-predict(ridge_pca,s=ridge_pca$lambda.min,newx=X_pca)
ridge_pca_mse<-mean((y_pca-ridge_pca_pred)^2)


ridge_pca_r2<-1-sum((y_pca-ridge_pca_pred)^2)/sum((y_pca-mean(y_pca))^2)
ridge_pca_rse<-sqrt(mean((y_pca-ridge_pca_pred)^2)) 
ridge_pca_mse<-mean((y_pca-ridge_pca_pred)^2)

n_pca<-length(y_pca)  # # samples
p_pca<-ncol(X_pca)  # # pca components
ridge_pca_aic<-n_pca*log(ridge_pca_mse)+2*p_pca 
ridge_pca_bic<-n_pca*log(ridge_pca_mse)+log(n_pca)*p_pca

```



#Ridge Linear, Ridge Poly, Ridge Interaction
```{r}

X<-model.matrix(Chance.of.Admit~ . ,data=data[,-1])
y <- data$Chance.of.Admit


ridge_linear<-cv.glmnet(X,y,alpha=0,nfolds=5)


poly_features<-model.matrix(~GRE.Score+I(GRE.Score^2)+TOEFL.Score+I(TOEFL.Score^2)+CGPA+I(CGPA^2)+SOP+LOR+Research,data=data[,-1])
ridge_poly<-cv.glmnet(poly_features, data$Chance.of.Admit, alpha = 0)


interaction_features<-model.matrix(~GRE.Score*University.Rating+CGPA*Research+TOEFL.Score+SOP+LOR,data=data[,-1])
ridge_interaction<-cv.glmnet(interaction_features, data$Chance.of.Admit, alpha = 0)


```


#Calculate all AIC BIC MSE R2 RSE
```{r}
# Ridge Model Metrics


ridge_r2<-1-sum((y-ridge_pred)^2)/sum((y-mean(y))^2)
ridge_rse<-sqrt(mean((y-ridge_pred)^2))
ridge_mse<-mean((y-ridge_pred)^2) 

ridge_coef <- coef(ridge, s = ridge$lambda.min)
n<-length(y)  # # samples
p<-length(ridge_coef)-1  # #coeff remove intercept
ridge_aic<-n*log(mean((y-ridge_pred)^2))+2*p 
ridge_bic<-n*log(mean((y-ridge_pred)^2))+log(n)*p 

######################################
#linear
linear_pred<-predict(linear_model,data[,-1])

linear_r2<-summary(linear_model)$r.squared
linear_rse<-summary(linear_model)$sigma
linear_mse<-mean((data$Chance.of.Admit-linear_pred)^2)
linear_aic<-AIC(linear_model)
linear_bic<-BIC(linear_model)

#######################################
#poly
poly_pred<-predict(poly_model,data)

poly_r2<-1-sum((data$Chance.of.Admit-poly_pred)^2)/sum((data$Chance.of.Admit-mean(data$Chance.of.Admit))^2)
poly_rse<-sqrt(mean((data$Chance.of.Admit-poly_pred)^2))
poly_mse<-mean((data$Chance.of.Admit-poly_pred)^2) 
poly_aic<-AIC(poly_model)
poly_bic<-BIC(poly_model)

########################################
# Interaction Model Metrics
interaction_pred<-predict(inter_model,data[,-1])

interaction_r2<- 1-sum((data$Chance.of.Admit-interaction_pred)^2)/sum((data$Chance.of.Admit-mean(data$Chance.of.Admit))^2)
interaction_rse<-sqrt(mean((data$Chance.of.Admit-interaction_pred)^2))
interaction_mse<-mean((data$Chance.of.Admit-interaction_pred)^2)  
interaction_aic<-AIC(inter_model)
interaction_bic<-BIC(inter_model)

########################################
#Ridge liner
ridge_linear_pred<-predict(ridge_linear,s=ridge_linear$lambda.min,newx = X)

ridge_linear_r2<- 1-sum((data$Chance.of.Admit-ridge_linear_pred)^2)/sum((data$Chance.of.Admit-mean(data$Chance.of.Admit))^2)
ridge_linear_mse<-mean((data$Chance.of.Admit-ridge_linear_pred)^2)
ridge_linear_rse<-sqrt(ridge_linear_mse)

n<-nrow(data)# #samples
p<-ncol(data[,-c(1, 9)])##features
ridge_linear_aic<-n*log(ridge_linear_mse)+2*p
ridge_linear_bic<-n*log(ridge_linear_mse)+log(n)*p
########################################
#Ridge poly
poly_features<-model.matrix(~GRE.Score+I(GRE.Score^2)+TOEFL.Score+I(TOEFL.Score^2)+CGPA+I(CGPA^2)+SOP+LOR+Research,data[,-1])

ridge_poly<-cv.glmnet(poly_features,data$Chance.of.Admit, alpha = 0)

ridge_poly_pred<- predict(ridge_poly,s=ridge_poly$lambda.min,newx=poly_features)

ridge_poly_r2<- 1-sum((data$Chance.of.Admit-ridge_poly_pred)^2)/sum((data$Chance.of.Admit-mean(data$Chance.of.Admit))^2)
ridge_poly_mse<-mean((data$Chance.of.Admit-ridge_poly_pred)^2)
ridge_poly_rse<-sqrt(ridge_poly_mse)

p <- ncol(poly_features)##features
ridge_poly_aic<-n*log(ridge_poly_mse)+2*p
ridge_poly_bic<-n*log(ridge_poly_mse)+log(n)*p
########################################
#Ridge interact
interaction_features<-model.matrix(~GRE.Score*University.Rating+CGPA*Research+TOEFL.Score + SOP + LOR, data)

ridge_interaction<-cv.glmnet(interaction_features,data$Chance.of.Admit,alpha = 0)

ridge_interaction_pred<-predict(ridge_interaction, s = ridge_interaction$lambda.min, newx=interaction_features)


ridge_interaction_r2<-1 -sum((data$Chance.of.Admit-ridge_interaction_pred)^2)/sum((data$Chance.of.Admit-mean(data$Chance.of.Admit))^2)
ridge_interaction_mse<-mean((data$Chance.of.Admit-ridge_interaction_pred)^2)
ridge_interaction_rse<-sqrt(ridge_interaction_mse)

p <- ncol(interaction_features) 
ridge_interaction_aic<-n*log(ridge_interaction_mse)+2*p
ridge_interaction_bic<-n*log(ridge_interaction_mse)+log(n)*p
########################################




results <- data.frame(
  Model = c("Linear", "Polynomial","Interaction","Ridge", "Ridge Linear","Ridge Polynomial","Ridge Interaction","PCA Linear", "PCA + Ridge"),
  R2 = c(linear_r2, poly_r2, interaction_r2, ridge_r2, ridge_linear_r2, ridge_poly_r2, ridge_interaction_r2, pca_r2, ridge_pca_r2),
  MSE = c(linear_mse, poly_mse, interaction_mse, ridge_mse, ridge_linear_mse, ridge_poly_mse, ridge_interaction_mse, pca_mse, ridge_pca_mse),
  RSE = c(linear_rse, poly_rse, interaction_rse, ridge_rse, ridge_linear_rse, ridge_poly_rse, ridge_interaction_rse, pca_rse, ridge_pca_rse),
  AIC = c(linear_aic, poly_aic, interaction_aic, ridge_aic, ridge_linear_aic, ridge_poly_aic, ridge_interaction_aic, pca_aic, ridge_pca_aic),
  BIC = c(linear_bic, poly_bic, interaction_bic, ridge_bic, ridge_linear_bic, ridge_poly_bic, ridge_interaction_bic, pca_bic, ridge_pca_bic)
)


print(results)




```

#plot
```{r}

#numeric vector
pca_pred<-as.numeric(pca_pred)
ridge_pca_pred<-as.numeric(ridge_pca_pred)

#PCA linear graph
ggplot(data=data.frame(Actual = data$Chance.of.Admit, Predicted = pca_pred), aes(x = Actual, y = Predicted)) +geom_point(color = "blue", alpha = 0.6) + geom_abline(intercept=0, slope = 1, linetype = "dashed", color = "red", size = 1) +labs(title = "PCA Linear Model", x = "Actual Chance of Admit", y = "Predicted Chance of Admit") +
  theme_minimal()

#PCA ridge graph
ggplot(data=data.frame(Actual = data$Chance.of.Admit, Predicted = ridge_pca_pred), aes(x = Actual,y=Predicted))+geom_point(color = "blue", alpha = 0.6)+geom_abline(intercept=0, slope = 1, linetype = "dashed", color = "red", size = 1) + labs(title = "PCA + Ridge Model",x ="Actual Chance of Admit", y = "Predicted Chance of Admit") +
  theme_minimal()


```
```{r}
#Cross Validation to find overfit and genralization

#PCA components
pca_result<-prcomp(num_features, scale.=TRUE)
pca_data<-as.data.frame(pca_result$x[, 1:3])  
colnames(pca_data)<-c("PC1","PC2","PC3")
pca_data$Chance.of.Admit <- data$Chance.of.Admit 


X_pca<-as.matrix(pca_data[, -4])  #main data
y_pca<-pca_data$Chance.of.Admit  #target

#Cross validataion
ridge_pca_cv <- cv.glmnet(X_pca, y_pca, alpha = 0, nfolds = 5)  # 5 folds

#best lambda by cv
cat("Best Lambda from PCA Ridge Model:",ridge_pca_cv$lambda.min,"\n")

# plot误差曲线
plot(ridge_pca_cv)
title("Cross-Validation Error for PCA+Ridge")

#train
ridge_pca_train_pred<-predict(ridge_pca_cv, s = ridge_pca_cv$lambda.min,newx=X_pca)

#calculate train residual
ridge_pca_train_mse<-mean((y_pca - ridge_pca_train_pred)^2)
cat("Training MSE for PCA + Ridge:",ridge_pca_train_mse)


# test resudial
ridge_pca_val_mse<-min(ridge_pca_cv$cvm) 
cat("Validation MSE for PCA + Ridge:",ridge_pca_val_mse)


#visualization
error_df<-data.frame(
  Type=c("Training MSE", "Validation MSE"),
  MSE=c(ridge_pca_train_mse,ridge_pca_val_mse)
)

library(ggplot2)
ggplot(error_df, aes(x = Type, y = MSE, fill = Type))+geom_bar(stat = "identity", color = "black")+labs(title = "Training vs Validation MSE for PCA + Ridge",x="Error Type", y = "Mean Squared Error") +theme_minimal()+scale_fill_manual(values = c("skyblue", "orange"))


```







#CGPA vs chance存在非线性关系尤其在高分段有饱和 GRE和TOEFL也是 CGPA，TOEFL，GRE有非线性关系

#从 SOP 和 LOR 的散点图来看，这些特征与 Chance of Admit 之间的关系较为分散，可能说明它们的影响较弱。

#从柱状图中可以看出有research的和university rating高的有更高的chance of admit,
#所以引入交互项

#整个代码没有严重的异常值不需要处理异常值，LOR有一个值是1是一个异常值但是我觉得是真实情况不用处理

#分布比较集中，应该不需要transform

#确定分类变量对录取概率的影响，初步探索数据特征

#SOP增长可能是非线性得，可能需要使用多项式回归

#LOR与其他变量相关性较低，或许是相对独立因素

#we can see that CGPA and GRE score have highest correlation with chance of admit,说明它们对录取概率的影响可能非常显著。toefl也是

#这些特征可以作为主要预测变量，甚至可能在多项式模型中加入非线性项

#LOR和SOP与其他特征的相关性较低，可能因为他比较独立

#通过逻辑推测，GRE和TOEFL可能存在交互关系，因为都反应申请人的学术能力

#GRE和TOEFL相关性高，CGPA和GRE和TOEFL都很高，有可能存在多重共线，后续会考虑PCA和Ridge



#根据qq图怀疑残差不是正态分布，使用shapiro test进行检测确定残差不是正态分布,这对推断如t test，CI会产生影响
#但是如果我们的主要目标是预测而不是推断，正态性的偏离可能不会显著影响模型预测能力
#从qq图来看，残差得偏离主要集中在尾部，实际上轻微的尾部偏离对线性回归的影响可能并不大
#如果有顾虑可以考虑transform data
#transform 后结果更差
#However, given the nature of the deviation and its mild impact on overall model performance, such adjustments may not be critical.
#adjusted R^2 us actually high in predictive accuracy, so may good to not transform or looking for new models.







