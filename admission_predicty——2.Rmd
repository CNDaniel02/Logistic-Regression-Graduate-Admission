---
title: "Final Project"
output: html_document
date: "2024-12-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
library(readr)
Admission <- read_csv("Admission_Predict_Ver1.1.csv")
View(Admission)
```

```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
library(readr)
library(gmodels)
library(GGally)
library(caret)
library(ggcorrplot)
```



```{r}
# Load necessary library
library(tidyr)

# Load the dataset
data <- read.csv("Admission_Predict_Ver1.1.csv")

print(sum(is.na(data)))

print(sum(duplicated(data)))



# 绘制连续变量的直方图和箱线图
par(mfrow = c(2, 2)) # 设置图形布局
for (col in c("GRE.Score", "TOEFL.Score", "CGPA")) {
  hist(data[[col]], main = paste("Histogram of", col), xlab = col, col = "skyblue", border = "black")
  boxplot(data[[col]], main = paste("Boxplot of", col), ylab = col, col = "orange")
}

# 绘制分类变量的柱状图和箱线图
# 设置图形排列
par(mfrow = c(2, 2))
barplot(table(data$University.Rating), main = "University Rating Distribution", col = "lightblue")
boxplot(data$Chance.of.Admit ~ data$University.Rating, main = "Chance of Admit by University Rating",
        xlab = "University Rating", ylab = "Chance of Admit", col = "lightgreen")




# Histogram and boxplot for Research
barplot(table(data$Research), main = "Barplot of Research", col = "skyblue", 
        xlab = "Research (0 = No, 1 = Yes)", ylab = "Frequency")
boxplot(data$Chance.of.Admit ~ data$Research, main = "Research vs Chance of Admit", 
        xlab = "Research", ylab = "Chance of Admit", col = c("lightblue", "lightgreen"))

# Histogram and boxplot for SOP
hist(data$SOP, main = "Histogram of SOP", col = "skyblue", xlab = "SOP")
boxplot(data$Chance.of.Admit ~ data$SOP, main = "SOP vs Chance of Admit", 
        xlab = "SOP", ylab = "Chance of Admit", col = "lightgreen")

# Histogram and boxplot for LOR
hist(data$LOR, main = "Histogram of LOR", col = "skyblue", xlab = "LOR")
boxplot(data$Chance.of.Admit ~ data$LOR, main = "LOR vs Chance of Admit", 
        xlab = "LOR", ylab = "Chance of Admit", col = "lightgreen")

# Reset图形排列
par(mfrow = c(1, 1))



# Convert the dataset to long format
long_data <- pivot_longer(data, cols = !"Serial.No.", names_to = "Variable", values_to = "Value")

# Convert categorical columns into factors
# Assuming "University Rating" and "Research" are categorical variables
data$University.Rating <- as.factor(data$University.Rating)
data$Research <- as.factor(data$Research)
#str(data)
```
#整个代码没有严重的异常值不需要处理异常值，LOR有一个值是1是一个异常值但是我觉得是真实情况不用处理
#分布比较集中，应该不需要transform

```{r}
par(mfrow = c(1, 2))
# Frequency distribution for University Rating
university_rating_freq <- table(data$University.Rating)
cat("Frequency Distribution for University Rating:\n")
print(university_rating_freq)

# Plot University Rating vs. Chance of Admit
plot(data$University.Rating, data$Chance.of.Admit, main="University Rating",
     xlab="University Rating", ylab="Chance of Admit", pch=19, col="red")




# Frequency distribution for Research
research_freq <- table(data$Research)
cat("Frequency Distribution for Research:\n")
print(research_freq)
# Create a boxplot to visualize the effect of Research on Chance of Admit
boxplot(data$Chance.of.Admit ~ data$Research, 
        main = "Research",
        xlab = "Research Experience (0 = No, 1 = Yes)", 
        ylab = "Chance of Admit",
        col = c("lightblue", "lightgreen"))
```
#确定分类变量对录取概率的影响，初步探索数据特征

```{r}
par(mfrow = c(2, 3))

# Plot GRE Score vs. Chance of Admit
plot(data$GRE.Score, data$Chance.of.Admit, main="GRE Score",
     xlab="GRE Score", ylab="Chance of Admit", pch=19, col="blue")

# Plot TOEFL Score vs. Chance of Admit
plot(data$TOEFL.Score, data$Chance.of.Admit, main="TOEFL Score",
     xlab="TOEFL Score", ylab="Chance of Admit", pch=19, col="green")

# Plot SOP vs. Chance of Admit
plot(data$SOP, data$Chance.of.Admit, main="SOP",
     xlab="SOP", ylab="Chance of Admit", pch=19, col="purple")

# Plot LOR vs. Chance of Admit
plot(data$LOR, data$Chance.of.Admit, main="LOR",
     xlab="LOR", ylab="Chance of Admit", pch=19, col="orange")

# Plot CGPA vs. Chance of Admit
plot(data$CGPA, data$Chance.of.Admit, main="CGPA",
     xlab="CGPA", ylab="Chance of Admit", pch=19, col="brown")

# Reset plotting area to default
par(mfrow = c(1, 1))
```
#SOP增长可能是非线性得，可能需要使用多项式回归


```{r}

# Load necessary libraries
library(readr)
library(ggcorrplot)

# Load the dataset

# Rename columns to make them more R-friendly
colnames(data) <- c("Serial_No", "GRE_Score", "TOEFL_Score", "University_Rating", "SOP", "LOR", "CGPA", "Research", "Chance_of_Admit")

# Select relevant columns for analysis
subset_data <- data[, c("GRE_Score", "TOEFL_Score", "SOP", "LOR", "CGPA")]

# Ensure all selected columns are numeric
subset_data <- subset_data %>% mutate(across(everything(), as.numeric))

# Plot histograms for each numerical variable
par(mfrow = c(2, 3))
for (col in colnames(subset_data)) {
  if (!is.null(subset_data[[col]])) {
    hist(subset_data[[col]], main = paste("Histogram of", col), xlab = col, col = "skyblue", border = "black")
  }
}

# Plot correlation matrix
cor_matrix <- cor(subset_data, use = "complete.obs")
ggcorrplot(cor_matrix, hc.order = TRUE, type = "lower", lab = TRUE)

```
#CGPA，TOEFL，GRE这些都高度相关，需要进行PCA
#LOR与其他变量相关性较低，或许是相对独立因素


```{r}
# 线性回归建模
model <- lm(Chance_of_Admit ~ GRE_Score + TOEFL_Score + CGPA + SOP + LOR + University_Rating + Research, data = data)

# 模型摘要
summary(model)

# 计算 VIF
library(car)
vif_values <- vif(model)
print(vif_values)


```
#R-squared, Adjusted R-squared足够高，预测误差较低，不需要使用PCA

```{r}
# 计算 Adjusted R-squared 和残差分布
summary(model)

# 可视化残差
par(mfrow = c(1, 2))
plot(model$fitted.values, model$residuals, 
     main = "Residuals vs Fitted", xlab = "Fitted Values", ylab = "Residuals")
hist(model$residuals, main = "Residual Histogram", xlab = "Residuals")
par(mfrow = c(1, 1))

qqnorm(model$residuals, main = "Q-Q Plot of Residuals")
qqline(model$residuals, col = "red")

shapiro.test(model$residuals)


#transform --> Normality
data$dependent_variable_log <- log(data$Chance_of_Admit)

shapiro.test(log(data$Chance_of_Admit))

data$GRE_Score_scaled <- scale(data$GRE_Score)
data$TOEFL_Score_scaled <- scale(data$TOEFL_Score)
# Re-fitting the model with transformed variables
model_transformed <- lm(dependent_variable_log ~ GRE_Score_scaled + TOEFL_Score_scaled + CGPA + SOP + LOR + University_Rating + Research, data = data)
summary(model_transformed)

# Visualizing residuals for the new model
par(mfrow = c(1, 2))
plot(model_transformed$fitted.values, model_transformed$residuals, 
     main = "Residuals vs Fitted", xlab = "Fitted Values", ylab = "Residuals")
hist(model_transformed$residuals, main = "Residual Histogram", xlab = "Residuals")
par(mfrow = c(1, 1))
qqnorm(model_transformed$residuals, main = "Q-Q Plot of Residuals")
qqline(model_transformed$residuals, col = "red")

# Testing residuals' normality again
shapiro.test(model_transformed$residuals)

```
#根据qq图怀疑残差不是正态分布，使用shapiro test进行检测确定残差不是正态分布,这对推断如t test，CI会产生影响
#但是如果我们的主要目标是预测而不是推断，正态性的偏离可能不会显著影响模型预测能力
#从qq图来看，残差得偏离主要集中在尾部，实际上轻微的尾部偏离对线性回归的影响可能并不大
#如果有顾虑可以考虑transform data
#(transformed,感觉效果不大)
#However, given the nature of the deviation and its mild impact on overall model performance, such adjustments may not be critical.
#adjusted R^2 us actually high in predictive accuracy, so may good to not transform or looking for new models.

```{r}
model_poly <- lm(Chance_of_Admit ~ GRE_Score + TOEFL_Score + CGPA + poly(SOP, 2) + LOR + University_Rating + Research, data = data)
summary(model_poly)
#polynomial regression
```
```{r}
#model_interaction <- lm(Chance_of_Admit ~ GRE_Score*TOEFL_Score + University_Rating + CGPA + SOP + LOR * Research, data = data)
#summary(model_interaction)
#interaction model

# Generate a model with all pairwise interactions
model_all_interactions <- lm(Chance_of_Admit ~ (GRE_Score + TOEFL_Score + CGPA + SOP + LOR + University_Rating + Research)^2, data = data)
summary(model_all_interactions)
# Load necessary library
library(MASS)

# Perform stepwise regression to select significant interactions
model_step <- stepAIC(model_all_interactions, direction = "both")
summary(model_step)
# Load necessary library
library(boot)

# Define a cross-validation function
cv_error <- function(data, indices) {
  # Split data into training and test sets
  train_data <- data[indices, ]
  test_data <- data[-indices, ]
  
  # Fit the model using training data
  model <- lm(Chance_of_Admit ~ GRE_Score + TOEFL_Score + CGPA + SOP + LOR + University_Rating + Research + GRE_Score:TOEFL_Score + LOR:Research, data = train_data)
  
  # Predict on test data
  predictions <- predict(model, newdata = test_data)
  
  # Calculate Mean Squared Error (MSE)
  mse <- mean((test_data$Chance_of_Admit - predictions)^2)
  return(mse)
}

# Perform cross-validation
set.seed(123)  # For reproducibility
cv_results <- boot(data, statistic = cv_error, R = 100)  # R specifies the number of folds
print(cv_results)
```
```{r}
model_interaction <- lm(Chance_of_Admit ~ GRE_Score + TOEFL_Score + CGPA + SOP + LOR + 
                  University_Rating + Research + 
                  GRE_Score:CGPA + CGPA:LOR + CGPA:Research + SOP:University_Rating, data = data)
summary(model_interaction)

```



```{r}
library(caret)
pca <- preProcess(data[, c("GRE_Score", "TOEFL_Score", "CGPA", "SOP", "LOR")], method = "pca", pcaComp = 3)
pca_data <- predict(pca, data)
model_pca <- lm(Chance_of_Admit ~ PC1 + PC2 + PC3 + University_Rating + Research, data = cbind(data, pca_data))
summary(model_pca)
#PCA model
```


```{r}
library(caret)

# 定义模型
model_list <- list(
  base = lm(Chance_of_Admit ~ GRE_Score + TOEFL_Score + CGPA + SOP + LOR + University_Rating + Research, data = data),
  poly = lm(Chance_of_Admit ~ GRE_Score + TOEFL_Score + CGPA + poly(SOP, 2) + LOR + University_Rating + Research, data = data),
  interaction = model_interaction <- lm(Chance_of_Admit ~ GRE_Score + TOEFL_Score + CGPA + SOP + LOR + 
                  University_Rating + Research + 
                  GRE_Score:CGPA + CGPA:LOR + CGPA:Research + SOP:University_Rating, data = data)
)

# 定义交叉验证
cv_results <- lapply(model_list, function(model) {
  train_control <- trainControl(method = "cv", number = 100)
  train(Chance_of_Admit ~ ., data = data, method = "lm", trControl = train_control)
})

# 打印 RMSE 比较
sapply(cv_results, function(res) res$results$RMSE)

```

#根据Cross validation，不同的model并没有太大区别，决定使用linear regression


#接下来根据MSE和R^2发现reduced model和full model区别不大，可以使用reduced model？
```{r}
# Split the data into training and testing sets
set.seed(42)
trainIndex <- sample(1:nrow(data), 0.8 * nrow(data))
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# Fit a linear regression model
model <- lm(Chance.of.Admit ~ GRE.Score + TOEFL.Score + University.Rating + SOP + LOR + CGPA + Research, data = trainData)

# Summarize the model
summary(model)

# Make predictions on the test set
predictions <- predict(model, newdata = testData)

# Calculate Mean Squared Error (MSE)
mse <- mean((testData$Chance.of.Admit - predictions)^2)

# Calculate R-squared
rss <- sum((testData$Chance.of.Admit - predictions)^2)
tss <- sum((testData$Chance.of.Admit - mean(trainData$Chance.of.Admit))^2)
r2 <- 1 - (rss/tss)

# Print MSE and R-squared for the reduced model
print(paste("Full Model MSE:", mse))
print(paste("Full Model R-squared:", r2))
```

```{r}
# Split the data into training and testing sets
set.seed(42)
trainIndex <- sample(1:nrow(data), 0.8 * nrow(data))
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# Fit the reduced linear regression model excluding insignificant variables
reduced_model <- lm(Chance.of.Admit ~ GRE.Score + TOEFL.Score + LOR + CGPA + Research, data = trainData)

# Summarize the reduced model
summary(reduced_model)

# Make predictions on the test set using the reduced model
reduced_predictions <- predict(reduced_model, newdata = testData)

# Calculate Mean Squared Error (MSE) for the reduced model
reduced_mse <- mean((testData$Chance.of.Admit - reduced_predictions)^2, na.rm = TRUE)

# Calculate R-squared for the reduced model
rss <- sum((testData$Chance.of.Admit - reduced_predictions)^2, na.rm = TRUE)
tss <- sum((testData$Chance.of.Admit - mean(trainData$Chance.of.Admit, na.rm = TRUE))^2, na.rm = TRUE)
reduced_r2 <- 1 - (rss/tss)

# Print MSE and R-squared for the reduced model
print(paste("Reduced Model MSE:", reduced_mse))
print(paste("Reduced Model R-squared:", reduced_r2))

```